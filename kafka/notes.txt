To restart kafka after an upgrade:
  brew services restart kafka
Or, if you don't want/need a background service you can just run:
  /usr/local/opt/kafka/bin/kafka-server-start /usr/local/etc/kafka/server.properties

open ~/.bashrc  
open ~/.bash_profile
echo $PATH  
export PATH="/usr/local/opt/kafka/bin:$PATH"
ls /usr/local/opt/kafka/bin - commands
ls /usr/local/opt/kafka/libexec
ls /usr/local/opt/kafka/libexec/config
ls /usr/local/opt/kafka/libexec/logs - logs
cat /usr/local/opt/kafka/libexec/config/server.properties
- broker.id=0
- num.partitions=1
- # root directory for all kafka znodes.
    zookeeper.connect=localhost:2181
- # Timeout in ms for connecting to zookeeper
    zookeeper.connection.timeout.ms=18000

- anatomy
    - broker - where data resides it can be in one server or cluster of server
        - only appends data
        - per replica requires more storage
        - Each cluster
            - Leader - have one server as a controller (leader) or the first node
                - leader marks the other node as sync or out of sync. Out of sync node cannot be a succeding leader by default in case the leader fails.
        - Topic - think like tables - Data Management
            - a single partititon is only located on its single log
            - it can have one ore more partitions
            - It uses round robin to distributed leaders and replica topic
        - apache ZooKeeper
            - does the failover to which server in a cluster becomes the leader
            - each broker/server require a uniquer id and is registered to ZooKeeper
    - Producer - sends data (json, xml, avro(serial - preffered) ) to broker cluster's topic (somewhat like a table) which can be horizontal be split
    - Consumer - each consomer is assigned a topic
        - consumer group
            - have group id use to coordinate which topic in a group gets the data
            - first consumer in a group becomes a leader 
            - each consumer sends a heart beat to the group coordinator to let it know that is it is functioning

Setting Up Kafka
- zookeeper-server-start /usr/local/opt/kafka/libexec/config/zookeeper.properties

broker
- sudo vi /usr/local/opt/kafka/libexec/config/server.properties
    listeners=PLAINTEXT://localhost:9092
    auto.create.topics.enable=false
kafka-server-start /usr/local/opt/kafka/libexec/config/server.properties

How to create a topic ?
kafka-topics --create --topic test-topic --replication-factor 1 --partitions 4 --bootstrap-server localhost:9092

How to instantiate a Console Producer?
    Without Key
    kafka-console-producer --broker-list localhost:9092 --topic test-topic
    With Key
    kafka-console-producer --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"
How to instantiate a Console Consumer?
    Without Key
    kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning
    With Key
    kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"


Consumer Offset
Consumer offset behaves like a bookmark for the consumer to start reading messages from the point it left off
kafka-topics --bootstrap-server  localhost:9092 --list
- Output below:
__consumer_offsets - auto generated by kafka to takecare of offsets 
test-topic - created by user 

Consumer Groups
Consumer Groups are use for scallable message consumption
Each different application will have unique consumer group
- Kafka broker manages the consumer Groups
- Kafka broker acts as a group coordinator
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
- Output below: This will list all consumer group. If you run a consumer without group id, it will auto generate group id for you
console-consumer-<group_id>
How to instantiate a Console Consumer with a group ID?
Without Key
    kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --group <group-name>
With Key
    kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true" --group <group-name>
Sidenote: If you instantiate consumer with the same group id, then the consumer will split the partition data. You should create a separate group id per application 

Commit log
- Each partion gets a log file - ls /usr/local/opt/kafka/libexec/logs
- Retention Policy
    - Determines how long the message is retained?
    - Configured using the property log.retention.hours in server.properties file
    - Default retention hours is 168 Hours (7 Days)
- ls /usr/local/opt/kafka/libexec/config
    - configure server.properties 
        - log.dirs=/usr/local/var/lib/kafka-logs
        - log.dirs - where the logs files are located
        - each topic correspond to a folder 
            -  ls /usr/local/var/lib/kafka-logs/test-topic-0
                - Contents:
                    - .index
                    - .log 
                        - To view a commit log:
                            - kafka-run-class kafka.tools.DumpLogSegments --deep-iteration --files /usr/local/var/lib/kafka-logs/test-topic-0/00000000000000000000.log
                    - .timeindex
                    - partition.metadata
                    - leader-epoch-checkpoint
        - log.retention.hours=168 # The minimum age of a log file to be eligible for deletion due to age

Kafka as a Distributed Streaming System using Cluster
- Availability and Fault Tolerance
- Reliable Work Distribution
- Easily Scalable
- Handling concurency is fairly easy
- Kafka Cluster
    - Can have one or more broker
    - Manage by ZooKeeper
    - Creating a Cluster
        - Each broker should have each own server.properties for example: server-1.properties
            - broker.id=1
            - listeners=PLAINTEXT://localhost:9093
            - log.dirs=/tmp/kafka-logs-1
            - auto.create.topics.enable=false(optional)

